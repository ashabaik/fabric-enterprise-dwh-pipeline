# Enterprise Data Warehouse ETL Pipeline

## ğŸ“‹ Project Overview

A production-grade, end-to-end data warehouse pipeline built with **Microsoft Fabric** that orchestrates the complete data flow from source systems to a dimensional data warehouse. This solution implements a multi-stage ETL process with automated ingestion, transformation, loading, and audit tracking.

**Built for:** Large-scale e-commerce data warehouse operations  
**Processing Volume:** Millions of daily transactions across customer, product, sales, and inventory domains

## ğŸ¯ Business Problem

Enterprise organizations need a reliable, scalable solution to:
- Ingest data from multiple source systems into a centralized data warehouse
- Transform raw data into business-ready dimensional models (star schema)
- Load both dimension and fact tables in the correct sequence
- Track data refresh cycles for audit and SLA compliance
- Ensure data consistency and quality across all stages

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Source Systems (OLTP)                        â”‚
â”‚          (SQL Server, APIs, File Systems, etc.)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   STAGE 1: Full Ingestion (Src to Landing)                      â”‚
â”‚   â€¢ Extract data from multiple sources                           â”‚
â”‚   â€¢ Raw data loaded into Landing Zone                           â”‚
â”‚   â€¢ No transformations applied                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Landing Area (Staging Layer)                        â”‚
â”‚   â€¢ Raw, unprocessed data                                        â”‚
â”‚   â€¢ Temporary storage for ETL processing                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   STAGE 2: Dimension Loading (Landing to DWH DIM)               â”‚
â”‚   â€¢ Transform and load dimension tables                          â”‚
â”‚   â€¢ Apply business rules and data quality checks                â”‚
â”‚   â€¢ SCD Type 1/2 handling                                        â”‚
â”‚   â€¢ Load: DimCustomer, DimProduct, DimDate, etc.                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   STAGE 3: Fact Loading (Landing to DWH FACT)                   â”‚
â”‚   â€¢ Transform and load fact tables                               â”‚
â”‚   â€¢ Join with dimension keys                                     â”‚
â”‚   â€¢ Aggregate measures and metrics                               â”‚
â”‚   â€¢ Load: FactSales, FactInventory, FactOrders, etc.            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   STAGE 4: Audit & Monitoring (Timestamp Recording)             â”‚
â”‚   â€¢ Record last refresh timestamp                                â”‚
â”‚   â€¢ Update audit table: dbo.LAST_REFRESH                        â”‚
â”‚   â€¢ Enable SLA tracking and monitoring                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Data Warehouse (Production)                           â”‚
â”‚   Ready for BI Dashboards, Analytics, and Reporting             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Key Features

### ğŸ”„ Pipeline Orchestration
- **Sequential execution** with dependency management
- **Error handling** at each stage with retry logic
- **Parallel processing** where possible for performance optimization
- **Conditional execution** based on previous stage outcomes

### ğŸ“Š Data Warehouse Design
- **Star schema** with dimension and fact tables
- **Slowly Changing Dimensions (SCD)** support
- **Surrogate keys** for dimensional integrity
- **Data quality validations** at each stage

### ğŸ›¡ï¸ Production-Ready Features
- Comprehensive logging and monitoring
- Audit trail with timestamp tracking
- Failure notifications and alerting
- Data lineage and impact analysis
- Performance optimization and indexing

### ğŸ¯ Business Value
- **Centralized analytics** - Single source of truth for business intelligence
- **Real-time insights** - Fresh data for decision-making
- **Scalability** - Handles growing data volumes efficiently
- **Data governance** - Audit tracking and compliance support

## ğŸ”§ Technical Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Orchestration** | Microsoft Fabric Data Factory | Pipeline coordination and scheduling |
| **Storage** | Microsoft Fabric Lakehouse | Landing zone and staging storage |
| **Processing** | PySpark Notebooks | Data transformations and business logic |
| **Data Warehouse** | Microsoft Fabric Warehouse | Production dimensional model |
| **Monitoring** | Fabric Monitoring Tools | Pipeline execution tracking |

## ğŸ“ Pipeline Components

### 1. **Full_Ingestion Pipeline** (Src to Landing)
**Purpose:** Extract raw data from source systems

**Activities:**
- Connect to source databases (SQL Server, APIs, files)
- Copy data to Landing zone without transformations
- Validate data extraction completeness
- Log ingestion metrics

**Key Technologies:**
- Copy Data activities
- Parameterized connections
- Incremental/full load logic

### 2. **Full Load Landing to DWH (DIM)** Pipeline
**Purpose:** Load and transform dimension tables

**Activities:**
- Read from Landing zone
- Apply data quality rules and transformations
- Handle Slowly Changing Dimensions (SCD Type 1/2)
- Load into dimension tables:
  - `DimCustomer` - Customer master data
  - `DimProduct` - Product catalog
  - `DimDate` - Date dimension
  - `DimLocation` - Geographic data
  - `DimEmployee` - Employee information

**Key Technologies:**
- PySpark for complex transformations
- SQL for data quality checks
- Lookups for surrogate key assignment

### 3. **Full Load Landing to DWH (FACT)** Pipeline
**Purpose:** Load fact tables with business transactions

**Activities:**
- Read transactional data from Landing
- Join with dimension tables for key lookups
- Calculate derived measures and aggregations
- Load into fact tables:
  - `FactSales` - Sales transactions
  - `FactOrders` - Order details
  - `FactInventory` - Inventory movements
  - `FactRevenue` - Revenue metrics

**Key Technologies:**
- Star schema joins
- Aggregate functions
- Incremental load patterns

### 4. **Audit & Timestamp Recording**
**Purpose:** Track data refresh cycles

**Activities:**
- Query system date/time
- Apply timezone offset (UTC+3)
- Update `dbo.LAST_REFRESH` audit table
- Enable monitoring dashboards

## ğŸš€ Pipeline Execution Flow

```
START
  â”‚
  â”œâ”€â–º [Src to Landing] â”€â”€Successâ”€â”€â–º [Landing to DWH DIM]
  â”‚                                         â”‚
  â”‚                                    On Complete
  â”‚                                         â”‚
  â”‚                                         â–¼
  â”‚                              [Landing to DWH FACT]
  â”‚                                         â”‚
  â”‚                                     Success
  â”‚                                         â”‚
  â”‚                                         â–¼
  â”‚                                   [Record Timestamp]
  â”‚                                         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  END
```

### Dependency Logic:
1. **Stage 1 â†’ Stage 2:** Proceeds only on successful ingestion
2. **Stage 2 â†’ Stage 3:** Proceeds on any completion (success/failure) to ensure fact loading attempts
3. **Stage 3 â†’ Stage 4:** Proceeds only on successful fact loading for accurate audit

## ğŸ“Š Data Warehouse Schema

### Dimension Tables (DIM):
```sql
-- Example: Customer Dimension
DimCustomer (
    CustomerKey INT PRIMARY KEY,      -- Surrogate key
    CustomerID VARCHAR(50),            -- Business key
    CustomerName VARCHAR(200),
    Email VARCHAR(100),
    RegistrationDate DATE,
    CustomerType VARCHAR(50),
    IsActive BIT,
    -- SCD Type 2 columns
    EffectiveDate DATE,
    ExpiryDate DATE,
    IsCurrent BIT
)
```

### Fact Tables (FACT):
```sql
-- Example: Sales Fact
FactSales (
    SalesKey BIGINT PRIMARY KEY,
    DateKey INT FOREIGN KEY,
    CustomerKey INT FOREIGN KEY,
    ProductKey INT FOREIGN KEY,
    OrderID VARCHAR(50),
    Quantity INT,
    UnitPrice DECIMAL(10,2),
    TotalAmount DECIMAL(10,2),
    DiscountAmount DECIMAL(10,2),
    NetAmount DECIMAL(10,2)
)
```

### Audit Table:
```sql
-- Last Refresh Tracking
dbo.LAST_REFRESH (
    RefreshID INT IDENTITY PRIMARY KEY,
    PipelineName VARCHAR(100),
    RefreshTimestamp DATETIME,
    Status VARCHAR(50),
    RecordsProcessed BIGINT
)
```

## ğŸ”’ Security & Governance

- **Role-based access control** for pipeline execution
- **Sensitive data masking** in non-production environments
- **Audit logging** for compliance requirements
- **Data encryption** at rest and in transit
- **Connection string security** via Azure Key Vault

## ğŸ“ˆ Performance Optimizations

- **Partitioning strategies** for large fact tables
- **Indexing** on frequently queried columns
- **Parallel execution** where dependencies allow
- **Incremental loading** to minimize data movement
- **Columnstore indexes** for analytical queries

## ğŸ“ Learning Outcomes

This project demonstrates:
- âœ… Enterprise-scale ETL pipeline design
- âœ… Microsoft Fabric ecosystem proficiency
- âœ… Dimensional modeling (Kimball methodology)
- âœ… Pipeline orchestration and dependency management
- âœ… Data quality and governance implementation
- âœ… Production deployment best practices

## ğŸ“« Contact

**Ahmed Mohamed**
- GitHub: [@ashabaik](https://github.com/ashabaik)
- LinkedIn: www.linkedin.com/in/ahmed-mohamed-280481273
- Email: shabaik1996@gmail.com

---

## ğŸ“ License

This project is for educational and portfolio purposes.

---

**â­ If you found this helpful, please star this repository!**
